{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This notebook has been made to make the datasets. Please keep in mind that:__\n",
    "\n",
    "> This is only applicable if the provided raw original datasets are used (german, compas and drug).\n",
    "\n",
    "> Only run this once (or not), since we already included the recreated datasets. It gives insight in how the datasets are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "########################### GERMAN DATASET ###########################\n",
    "def recreate_german_dataset():\n",
    "    file_path = os.path.join(\".\", \"resources\", \"german.data\")\n",
    "    data = pd.read_csv(file_path, delim_whitespace=True, header=None)\n",
    "    \n",
    "    targets = data[data.columns[-1]] # TARGET labels\n",
    "    data = data.drop(20, axis=1) # drop targets before rescaling\n",
    "\n",
    "    # had to change the targets since the targets were [1,2]\n",
    "    targets = targets.replace({1:0, 2:1})\n",
    "    targets = pd.DataFrame(targets).rename(columns={targets.name:\"targets\"})\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Attribute 9 (in our dataset attribute and index 8, since we start at 0, which later becomes idx 0):\n",
    "    Personal status and sex \n",
    "    A91 : male : divorced/separated \n",
    "    A92 : female : divorced/separated/married \n",
    "    A93 : male : single \n",
    "    A94 : male : married/widowed \n",
    "    A95 : female : single \n",
    "    \"\"\"\n",
    "\n",
    "    ## Sex attribute binary\n",
    "    data[8] = data[8].replace({\"A91\": 0, \"A92\": 1, \"A93\": 0, \"A94\": 0, \"A95\":1})\n",
    "    group_labels = data[8]\n",
    "    group_labels = pd.DataFrame(group_labels).rename(columns={group_labels.name:\"group_labels\"})\n",
    "\n",
    "    ## Sensitive feature is sex - attribute 8, make that now index 0\n",
    "    sensitive_feature_idx = data.pop(8)\n",
    "    data.insert(0, 8, sensitive_feature_idx)\n",
    "    data = data.rename(columns={i:j for i,j in zip(data.columns, range(13))})\n",
    "\n",
    "    # One-hot encode all categorical variables\n",
    "    str_columns = []\n",
    "    not_str = []\n",
    "    for i in data.columns:\n",
    "        if type(data[i][0]) == str:\n",
    "            str_columns.append(i)\n",
    "        else:\n",
    "            not_str.append(i)\n",
    "\n",
    "    # Add one-hot encoded data to the data\n",
    "    dummies = pd.get_dummies(data[str_columns])\n",
    "    data = pd.concat([data[not_str], dummies], axis=1, join='inner') \n",
    "\n",
    "    # First rescale to mean = 0 and std = 1, before adding targets to df (otherwise targets would be rescaled as well)\n",
    "    for i in data.columns:\n",
    "        data[i] = preprocessing.scale(data[i])\n",
    "\n",
    "    # Add targets and group labels to df\n",
    "    dataset = pd.concat([data, targets, group_labels], axis=1, join='inner')\n",
    "\n",
    "    # Thereafter reshuffle whole dataframe \n",
    "    dataset = dataset.sample(frac=1, random_state=2).reset_index(drop=True)\n",
    "\n",
    "    # Split dataframe in 80-20%\n",
    "    train, test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Make variable with grouplabels \n",
    "    group_label_train = np.array([i[0] for i in train.loc[:, \"group_labels\":].to_numpy()])\n",
    "    group_label_test = np.array([i[0] for i in test.loc[:, \"group_labels\":].to_numpy()])\n",
    "    group_label = np.concatenate((group_label_train, group_label_test))\n",
    "    \n",
    "    # Drop the grouplabels from the train and test\n",
    "    train = train.drop(train.columns[-1], axis=1)\n",
    "    test = test.drop(test.columns[-1], axis=1)\n",
    "    \n",
    "\n",
    "    # At last make x and y\n",
    "    X_train = train.iloc[:, :-1].to_numpy() # exclude targets\n",
    "    X_test = test.iloc[:, :-1].to_numpy()\n",
    "\n",
    "    y_train = train.iloc[:, -1:].to_numpy() # targets only\n",
    "    y_train = np.array([i[0] for i in y_train])\n",
    "\n",
    "    y_test = test.iloc[:, -1:].to_numpy() # targets only\n",
    "    y_test = np.array([i[0] for i in y_test])\n",
    "\n",
    "    np.savez(os.path.join(\"data.npz\"), X_train=X_train, Y_train=y_train, X_test=X_test, Y_test=y_test)\n",
    "    np.savez(os.path.join(\"german_group_label.npz\"), group_label=group_label)\n",
    "######################################################################\n",
    "\n",
    "\n",
    "########################### COMPAS DATASET ###########################\n",
    "def recreate_compas_dataset():\n",
    "\n",
    "    data = pd.read_csv(os.path.join(\".\", \"resources\", \"compas-scores-two-years.csv\"))\n",
    "    targets = data[data.columns[-1]]\n",
    "    targets = pd.DataFrame(targets).rename(columns={targets.name:\"targets\"})\n",
    "\n",
    "    # Used columns as specified in the paper\n",
    "    used_cols = [\"sex\", \"juv_fel_count\", \"priors_count\", \"race\", \"age_cat\", \n",
    "                \"juv_misd_count\", \"c_charge_degree\", \"juv_other_count\"]\n",
    "\n",
    "    data = data[used_cols]\n",
    "    # Manually change the values male to 0 and female to 1\n",
    "    data[\"sex\"] = data[\"sex\"].replace({\"Male\":0, \"Female\":1})\n",
    "    group_labels = data[\"sex\"]\n",
    "    group_labels = pd.DataFrame(group_labels).rename(columns={group_labels.name:\"group_labels\"})\n",
    "    \n",
    "    # One-hot encode and add to data\n",
    "    str_columns = [i for i in data.columns if type(data[i][0]) == str]\n",
    "    not_str = [i for i in data.columns if type(data[i][0]) != str]\n",
    "    dummies = pd.get_dummies(data[str_columns])\n",
    "    data = pd.concat([data[not_str], dummies], axis=1, join='inner') \n",
    "\n",
    "    # First rescale to mean = 0 and std = 1, before adding targets to df (otherwise targets would be rescaled as well)\n",
    "    for i in data.columns:\n",
    "        data[i] = preprocessing.scale(data[i])\n",
    "\n",
    "\n",
    "    dataset = pd.concat([data, targets, group_labels], axis=1, join='inner')\n",
    "\n",
    "    # Thereafter reshuffle whole dataframe \n",
    "    dataset = dataset.sample(frac=1, random_state=2).reset_index(drop=True)\n",
    "\n",
    "    # Split dataframe in 80-20%\n",
    "    train, test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Make variable with grouplabels\n",
    "    group_label_train = np.array([i[0] for i in train.loc[:, \"group_labels\":].to_numpy()])\n",
    "    group_label_test = np.array([i[0] for i in test.loc[:, \"group_labels\":].to_numpy()])\n",
    "    group_label = np.concatenate((group_label_train, group_label_test))\n",
    "    \n",
    "    # Drop the grouplabels from the train and test\n",
    "    train = train.drop(train.columns[-1], axis=1)\n",
    "    test = test.drop(test.columns[-1], axis=1)\n",
    "\n",
    "    # At last make x and y\n",
    "    X_train = train.iloc[:, :-1].to_numpy() # exclude targets\n",
    "    X_test = test.iloc[:, :-1].to_numpy()\n",
    "\n",
    "    y_train = train.iloc[:, -1:].to_numpy() # targets only\n",
    "    y_train = np.array([i[0] for i in y_train])\n",
    "\n",
    "    y_test = test.iloc[:, -1:].to_numpy() # targets only\n",
    "    y_test = np.array([i[0] for i in y_test])\n",
    "\n",
    "    np.savez(os.path.join(\"compas_data.npz\"), X_train=X_train, Y_train=y_train, X_test=X_test, Y_test=y_test)\n",
    "    np.savez(os.path.join(\"compas_group_label.npz\"), group_label=group_label)\n",
    "######################################################################\n",
    "\n",
    "\n",
    "########################### DRUG DATASET ###########################\n",
    "def recreate_drug_dataset():\n",
    "    \n",
    "    file_path = os.path.join(\".\", \"resources\", \"drug_consumption.data\")\n",
    "    data = pd.read_csv(file_path, delimiter=\",\", header=None)\n",
    "\n",
    "    # Targets. In the real dataset it is attribute 21 (python goes from 0, thus 20 in our case).\n",
    "    targets = data.iloc[:, 20] \n",
    "    \n",
    "    # They only take the first 13 attributes. See below the column specifications.\n",
    "    data = data.iloc[:, :13] \n",
    "    \n",
    "    ## Sensitive feature is gender - attribute 3, make that now index 0\n",
    "    sensitive_feature_idx = data.pop(2)\n",
    "    data.insert(0, 2, sensitive_feature_idx)\n",
    "    data = data.rename(columns={i:j for i,j in zip(data.columns, range(13))})\n",
    "    \n",
    "    \"\"\"\n",
    "    Our column specifications \n",
    "    0 = Gender, 1 = ID, 2 = Age, 3 = Education, 4 = Country, 5 = Ethinicity, 6 = NScore, 7 = EScore,\n",
    "    8 = OScore, 9 = AScore, 10 = CScore, 11 = Impulsiveness, 12 = SS, 20 = TARGET\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Problem which can be solved:\n",
    "    * Seven class classifications for each drug separately.\n",
    "    * Problem can be transformed to binary classification by union of part of classes into one new class. \n",
    "    For example, \"Never Used\", \"Used over a Decade Ago\" form class \"Non-user\" and \n",
    "    all other classes form class \"User\".\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    CL0 Never Used\n",
    "    CL1 Used over a Decade Ago \n",
    "    CL2 Used in Last Decade \n",
    "    CL3 Used in Last Year \n",
    "    CL4 Used in Last Month \n",
    "    CL5 Used in Last Week \n",
    "    CL6 Used in Last Day \n",
    "    \"\"\"\n",
    "\n",
    "    ## had to change the targets since the targets were not binary\n",
    "    targets = targets.replace({\"CL0\":0, \"CL1\":1, \"CL2\":1, \"CL3\":1, \"CL4\":1, \"CL5\":1, \"CL6\":1})\n",
    "    targets = pd.DataFrame(targets).rename(columns={targets.name:\"targets\"})\n",
    "    \n",
    "    # make group labels\n",
    "    group_labels = {i:(1 if data[0][i] > 0 else 0) for i in data[0].index }\n",
    "    group_labels = pd.DataFrame.from_dict(group_labels, orient='index', columns={\"group_labels\"})\n",
    "    \n",
    "    # First rescale to mean = 0 and std = 1, before adding targets to df (otherwise targets would be rescaled as well)\n",
    "    for i in data.columns:\n",
    "        data[i] = preprocessing.scale(data[i])\n",
    "\n",
    "    dataset = pd.concat([data, targets, group_labels], axis=1, join='inner') \n",
    "\n",
    "    # Thereafter reshuffle whole dataframe \n",
    "    dataset = dataset.sample(frac=1, random_state=2).reset_index(drop=True)\n",
    "\n",
    "    # Split dataframe in 80-20%\n",
    "    train, test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Make variable with grouplabels\n",
    "    group_label_train = np.array([i[0] for i in train.loc[:, \"group_labels\":].to_numpy()])\n",
    "    group_label_test = np.array([i[0] for i in test.loc[:, \"group_labels\":].to_numpy()])\n",
    "    group_label = np.concatenate((group_label_train, group_label_test))\n",
    "    \n",
    "    # Drop the grouplabels from the train and test\n",
    "    train = train.drop(train.columns[-1], axis=1)\n",
    "    test = test.drop(test.columns[-1], axis=1)\n",
    "\n",
    "    # At last make x and y\n",
    "    X_train = train.iloc[:, :-1].to_numpy() # exclude targets\n",
    "    X_test = test.iloc[:, :-1].to_numpy()\n",
    "\n",
    "    y_train = train.iloc[:, -1:].to_numpy() # targets only\n",
    "    y_train = np.array([i[0] for i in y_train])\n",
    "\n",
    "    y_test = test.iloc[:, -1:].to_numpy() # targets only\n",
    "    y_test = np.array([i[0] for i in y_test])\n",
    "\n",
    "    # Just a check\n",
    "    # print(len(X_train), len(X_test), len(y_train), len(y_test), len(group_label) == len(y_train) + len(y_test))\n",
    "\n",
    "    np.savez(os.path.join(\"drug2_data.npz\"), X_train=X_train, Y_train=y_train, X_test=X_test, Y_test=y_test)\n",
    "    np.savez(os.path.join(\"drug2_group_label.npz\"), group_label=group_label)\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not run this, since this will overwrite our datasets \n",
    "\n",
    "# recreate_german_dataset()\n",
    "# recreate_compas_dataset()\n",
    "# recreate_drug_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
